{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c6d838e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook we visualize both the trainings statistics and the performance of the trained models for the lunar lander agent trained via i) the actor-critic algorithm and ii) deep Q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c39d63b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-10T14:32:59.033411Z",
     "start_time": "2022-04-10T14:32:59.027144Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gymnasium.wrappers.monitoring'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# for creating videos of the best models:\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmonitoring\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m video_recorder\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# to load the agent class, we add the directory where it is located to the\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# python path of this notebook:\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gymnasium.wrappers.monitoring'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import numpy as np\n",
    "import h5py\n",
    "import copy\n",
    "import itertools\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    'font.size': 25,\n",
    "    \"font.sans-serif\": \"computer modern\",\n",
    "})\n",
    "\n",
    "# for creating videos of the best models:\n",
    "import torch\n",
    "from gymnasium.wrappers.monitoring import video_recorder\n",
    "import gymnasium as gym\n",
    "#\n",
    "# to load the agent class, we add the directory where it is located to the\n",
    "# python path of this notebook:\n",
    "agent_dir = '../'\n",
    "import sys\n",
    "sys.path.append(agent_dir)\n",
    "import agent_class as agent\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "247c6e32",
   "metadata": {},
   "source": [
    "# Load training- and simulation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e49a9dd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/agent_*_training_data.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m algorithms \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactor-critic\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m             \u001b[38;5;241m1\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdqn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m             }\n\u001b[0;32m      7\u001b[0m template_dict \u001b[38;5;241m=\u001b[39m {value:{} \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mvalues()}\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "files = glob.glob('./data/agent_*_training_data.h5')\n",
    "\n",
    "algorithms = {0:'actor-critic',\n",
    "            1:'dqn',\n",
    "            }\n",
    "\n",
    "template_dict = {value:{} for value in algorithms.values()}\n",
    "template_dict_counts = {value:0 for value in algorithms.values()}\n",
    "\n",
    "files_parameters = copy.deepcopy(template_dict)\n",
    "files_training = copy.deepcopy(template_dict)\n",
    "files_execution_time = copy.deepcopy(template_dict)\n",
    "files_trajectories = copy.deepcopy(template_dict)\n",
    "\n",
    "counts = copy.deepcopy(template_dict_counts)\n",
    "failed = copy.deepcopy(template_dict_counts)\n",
    "\n",
    "training_n_episodes = copy.deepcopy(template_dict)\n",
    "training_n_epochs = copy.deepcopy(template_dict)\n",
    "training_n_steps_simulated = copy.deepcopy(template_dict)\n",
    "#\n",
    "training_episode_durations = copy.deepcopy(template_dict)\n",
    "training_episode_returns = copy.deepcopy(template_dict)\n",
    "training_execution_time = copy.deepcopy(template_dict)\n",
    "\n",
    "#\n",
    "traj_return_per_episode = copy.deepcopy(template_dict)\n",
    "traj_duration_per_episode = copy.deepcopy(template_dict)\n",
    "\n",
    "mean_return_per_episode = copy.deepcopy(template_dict)\n",
    "mean_duration_per_episode = copy.deepcopy(template_dict)\n",
    "\n",
    "worst_mean = {value:np.inf for value in algorithms.values()}\n",
    "best_mean = {value:-np.inf for value in algorithms.values()}\n",
    "worst_model = {value:0 for value in algorithms.values()}\n",
    "best_model = {value:0 for value in algorithms.values()}\n",
    "\n",
    "\n",
    "\n",
    "def fn_training_to_trajectories(filename):\n",
    "    return filename.replace('training_data','trajs').replace('h5','tar')\n",
    "\n",
    "def fn_training_to_parameters(filename):\n",
    "    return filename.replace('_training_data','').replace('h5','tar')\n",
    "\n",
    "def fn_training_to_execution_time(filename):\n",
    "    return filename.replace('training_data.h5','execution_time.txt')\n",
    "\n",
    "def fn_trajectories_to_training(filename):\n",
    "    return filename.replace('trajs','training_data')\n",
    "\n",
    "for i, file in enumerate(files):\n",
    "    #\n",
    "    if 'dqn' in file:\n",
    "        algorithm = 'dqn'\n",
    "    else:\n",
    "        algorithm = 'actor-critic'\n",
    "    #\n",
    "    with h5py.File(file, 'r') as hf:\n",
    "        current_return_per_episode = np.array(hf['epsiode_returns'][()] )\n",
    "        if len(current_return_per_episode) == 10000:\n",
    "            print('training failed for {0}'.format(file))\n",
    "            failed[algorithm] += 1\n",
    "            continue\n",
    "        else:\n",
    "            c = counts[algorithm]\n",
    "            files_training[algorithm][c] = file\n",
    "            files_execution_time[algorithm][c] = fn_training_to_execution_time(file)\n",
    "            files_trajectories[algorithm][c] = fn_training_to_trajectories(file)\n",
    "            files_parameters[algorithm][c] = fn_training_to_parameters(file)\n",
    "            #\n",
    "            training_epochs = np.array(hf['n_training_epochs'][()] )\n",
    "            training_n_epochs[algorithm][c] = training_epochs[-1]\n",
    "            #\n",
    "            n_episodes_ = len(hf['episode_durations'][()] )\n",
    "            training_n_episodes[algorithm][c] = n_episodes_\n",
    "            #\n",
    "            n_steps_simulated_ = np.array(hf['n_steps_simulated'][()] )\n",
    "            training_n_steps_simulated[algorithm][c] = n_steps_simulated_[-1]\n",
    "            #\n",
    "            training_episode_durations[algorithm][c] = np.array(hf['episode_durations'][()] )\n",
    "            training_episode_returns[algorithm][c] = current_return_per_episode\n",
    "            #\n",
    "            counts[algorithm] += 1\n",
    "    #\n",
    "    training_execution_time[algorithm][c] = \\\n",
    "                        np.loadtxt(files_execution_time[algorithm][c])\n",
    "    #\n",
    "    with h5py.File(files_trajectories[algorithm][c], 'r') as hf:\n",
    "        #\n",
    "        current_return_per_episode = np.array(hf['returns'][()] )\n",
    "        traj_return_per_episode[algorithm][c] = current_return_per_episode\n",
    "        #\n",
    "        current_duration_per_episode = np.array( hf['durations'][()] )\n",
    "        traj_duration_per_episode[algorithm][c] = current_duration_per_episode\n",
    "        #\n",
    "        current_mean_return_per_episode = np.mean(current_return_per_episode)\n",
    "        mean_return_per_episode[algorithm][c] = current_mean_return_per_episode\n",
    "        if current_mean_return_per_episode < worst_mean[algorithm]:\n",
    "            worst_model[algorithm] = c\n",
    "            worst_mean[algorithm] = current_mean_return_per_episode\n",
    "        if current_mean_return_per_episode > best_mean[algorithm]:\n",
    "            best_model[algorithm] = c\n",
    "            best_mean[algorithm] = current_mean_return_per_episode\n",
    "        #\n",
    "        current_mean_duration_per_episode = np.mean(current_duration_per_episode)\n",
    "        mean_duration_per_episode[algorithm][c] = current_mean_duration_per_episode\n",
    "\n",
    "output_string = (\"For algorithm {0}, we observe {1} failed trainings \"\n",
    "             \"out of {2} trainings in total. This is a failure rate of \"\n",
    "             \"{3:<3.1f}%\")\n",
    "for current_algorithm in algorithms.values():\n",
    "    current_failed = failed[current_algorithm]\n",
    "    current_count = counts[current_algorithm]\n",
    "    print(output_string.format(current_algorithm,current_failed,current_count,\n",
    "         current_failed/(current_failed+current_count)*100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e49e153",
   "metadata": {},
   "source": [
    "# Plot number of episodes needed for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912268fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {'actor-critic':'dodgerblue',\n",
    "                'dqn':'red'}\n",
    "\n",
    "def plot_training_stats(save=False):\n",
    "        # \n",
    "        fig,ax = plt.subplots(1,1,figsize=(7,4))\n",
    "        fig.suptitle('Episodes needed for training',\n",
    "                        y=1.02,fontsize=25)\n",
    "        #\n",
    "        mean_values = {}\n",
    "        #\n",
    "        for algorithm in algorithms.values():\n",
    "                current_array = np.array( list(training_n_episodes[algorithm].values()))\n",
    "\n",
    "                hist, bin_edges = np.histogram(current_array,density=True,\n",
    "                                                bins=50)\n",
    "                bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "                bin_width = bin_edges[1] - bin_edges[0]\n",
    "                \n",
    "                #\n",
    "                mean_values[algorithm] =  np.mean(current_array)\n",
    "                #\n",
    "                if algorithm == 'dqn':\n",
    "                        label=algorithm.upper()\n",
    "                else:\n",
    "                        label = algorithm\n",
    "                #\n",
    "                ax.bar(bin_centers,hist,width=bin_width,\n",
    "                        alpha=0.2,\n",
    "                        label='{0}, mean = {1:3.1f}'.format(label,\n",
    "                                              mean_values[algorithm] ),\n",
    "                        color=colors[algorithm])\n",
    "                \n",
    "                if True:\n",
    "                        ax.plot(bin_centers, hist,\n",
    "                                        color=colors[algorithm])\n",
    "                #\n",
    "                \n",
    "\n",
    "        ax.set_xlabel(r'$N_{\\mathrm{episodes}}$')\n",
    "        ax.set_ylabel(r'$P(N_{\\mathrm{episodes}})$')\n",
    "\n",
    "        ax.legend(loc='best',fontsize=17)\n",
    "\n",
    "        plt.show()\n",
    "        if save:\n",
    "                fig.savefig('training_n_episodes.pdf',bbox_inches='tight')\n",
    "                fig.savefig('training_n_episodes.png',bbox_inches='tight',\n",
    "                                dpi=300)\n",
    "        plt.close(fig)\n",
    "        #\n",
    "        return mean_values\n",
    "\n",
    "mean_training_episodes = plot_training_stats(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdbb201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_execution_time(save=False):\n",
    "        # \n",
    "        fig,ax = plt.subplots(1,1,figsize=(7,4))\n",
    "        fig.suptitle('Time needed for training',\n",
    "                        y=1.02,fontsize=25)\n",
    "        #\n",
    "        mean_training_times = {}\n",
    "        #\n",
    "        for algorithm in algorithms.values():\n",
    "                current_array = np.array( list(training_execution_time[algorithm].values()))\n",
    "\n",
    "                hist, bin_edges = np.histogram(current_array,density=True,\n",
    "                                                bins=50)\n",
    "                bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "                bin_width = bin_edges[1] - bin_edges[0]\n",
    "                \n",
    "                mean_training_times[algorithm] = np.mean(current_array)\n",
    "                #\n",
    "                if algorithm == 'dqn':\n",
    "                        label=algorithm.upper()\n",
    "                else:\n",
    "                        label = algorithm\n",
    "                #\n",
    "                ax.bar(bin_centers,hist,width=bin_width,\n",
    "                        alpha=0.2,\n",
    "                        label='{0}, mean = {1:3.1f}'.format(label,\n",
    "                                               mean_training_times[algorithm]),\n",
    "                        color=colors[algorithm])\n",
    "                \n",
    "                if True:\n",
    "                        ax.plot(bin_centers, hist,\n",
    "                                        color=colors[algorithm])\n",
    "\n",
    "        ax.set_xlabel(r'$t$ ($s$)')\n",
    "        ax.set_ylabel(r'$P(t)$')\n",
    "\n",
    "        ax.legend(loc='best',fontsize=17)\n",
    "\n",
    "        plt.show()\n",
    "        if save:\n",
    "                fig.savefig('training_execution_time.pdf',bbox_inches='tight')\n",
    "                fig.savefig('training_execution_time.png',bbox_inches='tight',\n",
    "                                dpi=300)\n",
    "        plt.close(fig)\n",
    "        #\n",
    "        return mean_training_times\n",
    "        #\n",
    "mean_training_times = plot_execution_time(save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d07ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ratio(dictionary_values):\n",
    "    for i, algorithm in algorithms.items():\n",
    "        #\n",
    "        if i == 0:\n",
    "            x = dictionary_values[algorithm]\n",
    "        else:\n",
    "            x /= dictionary_values[algorithm]\n",
    "    return x\n",
    "\n",
    "print('Ratio for mean number of training episodes:')\n",
    "print('(# training episodes for {0})/(# training episodes for {1})'.format(\n",
    "        algorithms[0],algorithms[1]),end='')\n",
    "print(' = {0:3.2f}'.format(get_ratio(mean_training_episodes)))\n",
    "\n",
    "print('Ratio for mean time needed for training:')\n",
    "print('(# training time for {0})/(# training time for {1})'.format(\n",
    "        algorithms[0],algorithms[1]),end='')\n",
    "print(' = {0:3.2f}'.format(get_ratio(mean_training_times)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8081c5c3",
   "metadata": {},
   "source": [
    "# Return distribution for trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ffe4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashes = {'actor-critic':[1,0],\n",
    "                'dqn':[3,3]\n",
    "                }\n",
    "linewidths = {'actor-critic':5,\n",
    "                'dqn':4\n",
    "                }\n",
    "\n",
    "best_models = {value:0 for value in algorithms.values()}\n",
    "\n",
    "def plot_returns(best_performing=False,save=False):\n",
    "        # \n",
    "        fig,ax = plt.subplots(1,1,figsize=(8,5))\n",
    "        #\n",
    "        for algorithm in algorithms.values():\n",
    "                if best_performing:\n",
    "                        best_mean = -np.inf\n",
    "                        for i, current_returns in traj_return_per_episode[algorithm].items():\n",
    "                                current_mean = np.mean(current_returns)\n",
    "                                if current_mean > best_mean:\n",
    "                                        i_best = i\n",
    "                                        mean_best = current_mean.copy()\n",
    "                        best_models[algorithm] = i_best\n",
    "                        current_array = traj_return_per_episode[algorithm][i_best]\n",
    "                        #\n",
    "                        xlims = [0,350]\n",
    "                        filename = 'return_distribution_best'\n",
    "                        N_bin_edges = 50\n",
    "                else:\n",
    "                        current_array = np.array( list(traj_return_per_episode[algorithm].values()))\n",
    "                        xlims = [-200,350]\n",
    "                        filename = 'return_distribution'\n",
    "                        N_bin_edges = 100\n",
    "                #\n",
    "                current_mean = np.mean(current_array)\n",
    "                print('{0}, full mean = {1:3.2f}'.format(algorithm,current_mean))\n",
    "                cutoff = -200\n",
    "                mask = (current_array < cutoff)\n",
    "                print('{0}, P(return < {1}) = {2:3.2f}%'.format(algorithm,cutoff,\n",
    "                        np.sum(mask)/np.prod(np.shape(mask))*100))\n",
    "                print('{0}, minimal observed reward = {1:3.3f}'.format(algorithm,\n",
    "                                np.min(current_array)))\n",
    "                #current_array = current_array[~mask]\n",
    "                #print(algorithm,'mean =',np.mean(current_array))\n",
    "\n",
    "                #\n",
    "                if algorithm == 'dqn':\n",
    "                        label=algorithm.upper()\n",
    "                else:\n",
    "                        label = algorithm\n",
    "                #\n",
    "                bin_edges = np.linspace(*xlims,num=N_bin_edges)\n",
    "                hist, bin_edges = np.histogram(current_array,density=True,\n",
    "                                                bins=bin_edges)\n",
    "                bin_centers = (bin_edges[1:] + bin_edges[:-1])/2\n",
    "                ax.plot(bin_centers,hist,\n",
    "                       lw=linewidths[algorithm],\n",
    "                       dashes=dashes[algorithm],\n",
    "                        label='{0}, mean = {1:3.1f}'.format(label,\n",
    "                                                np.mean(current_array)),\n",
    "                        color=colors[algorithm])\n",
    "\n",
    "        ax.set_xlabel(r'Return')\n",
    "        ax.set_ylabel(r'$P$(return)')\n",
    "        fig.suptitle(r'Probability density for return')\n",
    "        ax.set_xlim(*xlims)\n",
    "\n",
    "        ax.legend(loc='best',fontsize=17)\n",
    "\n",
    "        plt.show()\n",
    "        if save:\n",
    "                fig.savefig('{0}.pdf'.format(filename),bbox_inches='tight')\n",
    "                fig.savefig('{0}.png'.format(filename),bbox_inches='tight',\n",
    "                                dpi=300)\n",
    "        plt.close(fig)\n",
    "        #\n",
    "plot_returns(save=True)\n",
    "plot_returns(best_performing=True,save=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "308e84f4",
   "metadata": {},
   "source": [
    "# Videos of best-performing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2f34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/gym/issues/2433\n",
    "# Wrapper class that calls the \"event loop\" for us when rendering.\n",
    "class PyGameWrapper(gym.Wrapper):\n",
    "    def render(self, **kwargs):\n",
    "        retval = self.env.render( **kwargs)\n",
    "        for event in pygame.event.get():\n",
    "            pass\n",
    "        return retval\n",
    "\n",
    "\n",
    "def create_video(filename_model,episode=-1,N_runs=10,\n",
    "                filename='video'):\n",
    "    #\n",
    "    # load model\n",
    "    input_dictionary = torch.load(open(filename_model,'rb'))\n",
    "    dict_keys = list(input_dictionary.keys())\n",
    "    if episode == -1:\n",
    "        dictionary_key = dict_keys[-1]\n",
    "    else:\n",
    "        dictionary_key = episode\n",
    "    #\n",
    "    input_dictionary_ = input_dictionary[dictionary_key]\n",
    "    #\n",
    "    N_state = 8\n",
    "    N_actions = 4\n",
    "    parameters = {'N_state':N_state,\n",
    "             'N_actions':N_actions}\n",
    "    if 'dqn' in filename_model:\n",
    "        my_agent = agent.dqn(parameters=parameters)\n",
    "    else:\n",
    "        my_agent = agent.actor_critic(parameters=parameters)\n",
    "    my_agent.load_state(state=input_dictionary_)\n",
    "    #\n",
    "    env = gym.make('LunarLander-v3', render_mode=\"rgb_array\")\n",
    "    video = video_recorder.VideoRecorder(env, './{0}.mp4'.format(filename))\n",
    "    for j in range(N_runs):\n",
    "        state, info = env.reset()\n",
    "\n",
    "        total_reward = 0\n",
    "        for i in itertools.count():\n",
    "            #env.render()\n",
    "            video.capture_frame()\n",
    "\n",
    "            action = my_agent.act(state)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print(i,total_reward)\n",
    "    video.close()\n",
    "    env.close()\n",
    "\n",
    "for algorithm, index in best_models.items():\n",
    "    print(\"Creating video for algorithm {0}\".format(algorithm))\n",
    "    filename_model = files_parameters[algorithm][index]\n",
    "    filename_video = 'video_{0}'.format(algorithm.replace(',','').replace(\n",
    "                                ' ','_'))\n",
    "    create_video(filename_model=filename_model,\n",
    "                    filename=filename_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc64653",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b3413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
